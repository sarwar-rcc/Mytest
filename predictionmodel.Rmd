---
title: "featureselection"
author: "sarwar"
date: "July 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description of the work
```{r}
#Given dataset is a size of I GB individual log in data with 21 individual parameters such as timestamp, epoch time, user ID, individual cloud, connection status, MSISDN login submission failure status, user activation submission status, user authorization status, email verification status, email verification failure status, MSISDN register page, MSISDN register page submission failure status, availability, user denied authorization status, user succeeded authentication, email login submission status,  MSISDN login submission status, new user creation status, and user activation submission status.

#By studying observable data following information can be analyzed from the given dataset
#1.How many connected user in a certain time period would be succesfully authorized or failed
#2. Epoc time share ratio
#3. Availability of a user in a certain timestamp or time period
#4. Probability of new user creation in comeing timestamps
#5. Probaility of user to be connected in a certain period
#6. Probability of new user activation submitted
```

## Process of the work
```{r}
#To develop a prediction model, we need to do the following steps:

#1. What are the individual data mean in the given data set?
#2. Using Features selections find the correlation between the data in the data set
#3. What are the dependent and independent data in the data set?
#4. Create linear regression model
#5. Find the coefficients and residual
#6. Determine the residual is appropriate or not?
#7. Predict value using the regression modle
#8. Cross validate the prediction model

```
##Model Evaluation
```{r}
#We can check if a model works well for data in many different ways. We pay great attention to regression results, such as slope coefficients, p-values, or R2 that tell us how well a model represents given data. Residuals could also show how poorly a model represents data. Residuals are leftover of the outcome variable after fitting a model (predictors) to data and they could reveal unexplained patterns in the data by the fitted model. Using this information, not only we could check if linear regression assumptions are met, but we could improve the proposed model in an exploratory way.
```

```{r packages}
 require(leaps)
```

```{r packages2}
 require(MASS)
```

###Data reading
```{r Data Processing}
rdata <- read.csv(file.choose(), header=T)

```

##Create Data frame
```{r transfer data to R object}
dataobj <- rdata[,c("timeStamp","epoch_time","UserID","MY_CLOUD","CONNECT","msisdn_login_submitted_failed","user_activation_submitted_failed","user_allowed_authorization","email_verification_submitted","email_verification_submitted_failed","msisdn_register_page","msisdn_register_submitted_failed","msisdn_register_submitted","email_login_submitted_faile","availability","user_denied_authorization","user_succeeded_authentication","email_login_submitted","msisdn_login_submitted","new_user_created","user_activation_submitted")]

summary(dataobj)
```

```{r transfer data to individual r object}
Timestamp <- rdata[,c("timeStamp")]
UserID <- rdata[,c("UserID")]
Connect <- rdata[,c("CONNECT")]
Epoch_time <- rdata[,c("epoch_time")]
user_allowed_authorization <- rdata[,c("user_allowed_authorization")]
email_login_submitted <- rdata[,c("email_login_submitted")]
user_succeeded_authentication <- rdata[,c("user_succeeded_authentication")]

```
### Feature selection and corelation between the most observable data
```{r feature selection}
reg=regsubsets(timeStamp ~ epoch_time +UserID + CONNECT + user_allowed_authorization + user_succeeded_authentication + msisdn_login_submitted,  data=dataobj, method="backward")
```

```{r}
#Explanation of the result of summary
#for the best subset of size 1, it puts 1 star next to the variable that's in the best subset of size 1.
#for the best subset of size 2, it puts 2 stars next to the variables that are in the best subset of size 2
#..
##for the best subset of size n, it puts n stars next to the variables that are in the best subset of size n
summary(reg)
```

```{r}
library("lattice")
```

```{r}
# The XY Graph (a Scatter Plot) is a flexible graphing reporting tool for plotting data pairs. It is very useful when analyzing the correlation between readings from two or more sensors.  The XY-Graph may be used as an alternative to wind rose presentation, and to systematically compare sensor data when seeking hidden correlation.

xyplot(UserID ~ Timestamp, data = rdata,
  xlab = "Number of User on log scale",
  ylab = "Timestamp (seconds) on log scale",
  main = "User Log in time"
)

```

```{r}
# hexbinplot creates a hexbin object from data supplied to it and plots it usinggrid.hexagons. To make panels comparable, all panels have the same maxcnt value, by default the maximum count over all panels. The hex-binned heat map reminds me of the effect of using transparency to overcome overplotting. You see a faint light-blue haze where the density of points is low. Dark colors indicate regions for which the density of points is high (more than 50 points per bin, for this example).
# This hex-binned diagram can be a useful alternative to a scatter plot when the scatter plot suffers from overplotting or when you want to estimate the density of the observations.

library("hexbin")
hexbinplot(UserID ~ epoch_time, dataobj,
          scales = list(x = list(log = 10, equispaced.log = FALSE)),
           aspect = 1, bins=50)
```

```{r}
# diagnostic plots: In order to diagonise the linear model 
layout(matrix(c(1,2,3,4),8,8)) # optional 4 graphs/page 
plot(reg)

```

```{r}
plot(reg,scale="adjr2")
```
```{r}
# Scatterplot Matrices from the glus Package 
library(car)
scatterplot.matrix(~timeStamp+UserID+epoch_time|CONNECT, data=dataobj,
  	main="User's Connection status")
```
```{r}
# Scatterplot Matrices from the glus Package 
library(car)
scatterplot.matrix(~timeStamp+MY_CLOUD|UserID, data=dataobj,
  	main="User's service status")
```
#Data Modeling

```{r linear modeling}
con.mod1 = lm(UserID ~ Connect + Timestamp, data = rdata)
summary(con.mod1)
# Summary Description
# The estimates for the model intercept is 6.097e+14 and the coefficient measuring the slope of the relationship with CONNECT status is -3.746e+10 and information about standard errors of these estimates is also provided in the Coefficients table. We see that the test of significance of the model coefficients is also summarised in that table so we can see that there is strong evidence that the coefficient is significantly different to zero – as the snout vent length increases so does the weight.
```

```{r}
#A plot of the residuals against fitted values is used to determine whether there are any systematic patterns, such as over estimation for most of the large values or increasing spread as the model fitted values increase.
xyplot(resid(con.mod1) ~ fitted(con.mod1),
  xlab = "Fitted Values",
  ylab = "Residuals",
  main = "Residual Diagnostic Plot",
  panel = function(x, y, ...)
  {
    panel.grid(h = -1, v = -1)
    panel.abline(h = 0)
    panel.xyplot(x, y, ...)
  }
)
```

```{r}
#The plot is probably ok but there are more cases of positive residuals and when we consider a normal probability plot we see that there are some deficiencies with the model:
#The function resid extracts the model residuals from the fitted model object. The plot is shown here:
qqmath( ~ resid(con.mod1),
  xlab = "Theoretical Quantiles",
  ylab = "Residuals"
)

#This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good if residuals are lined well on the straight dashed line.
```

## Logistic Regression Model
```{r}
# Since most of the dependent data are binary i.e. 0 or 1, logistic regression model would be more fit
```

```{r}
library(aod)
library(ggplot2)
library(Rcpp)
```

```{r}
# Previously we summarized the data set. Now to get the standard deviations, we use sapply to apply the sd function to manually observable variable in the dataset.
sapply(rdata,sd)
```

```{r}
## two-way contingency table of categorical outcome and predictors
## we want to make sure there are not 0 cells
xtabs(~ UserID + CONNECT, data = rdata)
```

```{r}
mylogit <- glm(CONNECT ~ UserID + Timestamp + user_allowed_authorization, data = dataobj, family = "binomial")
summary(mylogit)

```

